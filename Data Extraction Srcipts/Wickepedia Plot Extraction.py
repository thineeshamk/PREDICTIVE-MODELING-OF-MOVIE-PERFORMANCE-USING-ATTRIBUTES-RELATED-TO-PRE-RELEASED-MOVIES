# Remeber all the paths names and attribute names are from my side you can change those according to yours.
# ====================================
# INSTALL DEPENDENCIES (run once)
# ====================================
!pip install pandas openpyxl requests beautifulsoup4 tqdm -q

# ====================================
# IMPORTS
# ====================================
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import quote
from tqdm import tqdm
import os

# ====================================
# CONFIGURATION
# ====================================
WIKI_API = "https://en.wikipedia.org/w/api.php"
HEADERS = {"User-Agent": "MoviePlotExtractor/1.2 (research-use) add_yours@example.com"}
SLEEP_BETWEEN_CALLS = 1.0
CHECKPOINT_INTERVAL = 50
MIN_PLOT_CHARS = 120

# ====================================
# UTILITIES
# ====================================
def clean_title(title: str) -> str:
    if not isinstance(title, str):
        return ""
    return re.sub(r'^\s*\d+\.\s*', '', title).strip()

def wiki_search_best_title(query: str, max_results: int = 5):
    params = {"action": "query", "list": "search", "srsearch": query, "format": "json", "srlimit": max_results}
    try:
        r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=10)
        data = r.json()
        items = data.get("query", {}).get("search", [])
        if not items:
            return None
        return items[0]["title"]
    except Exception:
        return None

def wiki_get_sections(page_title: str):
    params = {"action": "parse", "page": page_title, "prop": "sections", "format": "json", "redirects": True}
    try:
        r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=10)
        data = r.json()
        return data.get("parse", {}).get("sections", []) or []
    except Exception:
        return []

def wiki_get_section_text(page_title: str, section_index: str):
    params = {"action": "parse", "page": page_title, "prop": "text", "section": section_index, "format": "json", "redirects": True}
    try:
        r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=12)
        html = r.json().get("parse", {}).get("text", {}).get("*", "")
        if not html:
            return None
        soup = BeautifulSoup(html, "html.parser")
        text = soup.get_text(" ", strip=True)
        text = re.sub(r'\[\d+\]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text if text else None
    except Exception:
        return None

def extract_plot_from_html(url: str):
    try:
        r = requests.get(url, headers=HEADERS, timeout=12)
        if r.status_code != 200:
            return None
        soup = BeautifulSoup(r.text, "html.parser")
        # Search headings with id containing 'plot'
        span = soup.find("span", id=lambda v: v and "plot" in v.lower())
        if span:
            header = span.find_parent(["h2", "h3"])
            if header:
                paras = []
                for sib in header.find_next_siblings():
                    if sib.name in ["h2", "h3"]:
                        break
                    if sib.name == "p":
                        txt = sib.get_text(" ", strip=True)
                        txt = re.sub(r'\[\d+\]', '', txt)
                        if len(txt) > 30:
                            paras.append(txt)
                if paras:
                    return " ".join(paras)
        # Otherwise, scan all headings
        for header in soup.find_all(["h2", "h3"]):
            header_text = header.get_text(" ", strip=True).lower()
            if any(k in header_text for k in ["plot", "synopsis", "summary", "plot summary"]):
                paras = []
                for sib in header.find_next_siblings():
                    if sib.name in ["h2", "h3"]:
                        break
                    if sib.name == "p":
                        txt = sib.get_text(" ", strip=True)
                        txt = re.sub(r'\[\d+\]', '', txt)
                        if len(txt) > 30:
                            paras.append(txt)
                if paras:
                    return " ".join(paras)
        return None
    except Exception:
        return None

# ====================================
# MASTER FUNCTION TO FIND PLOT
# ====================================
def find_plot_for_title(raw_title: str, year=None):
    title = clean_title(raw_title)
    year_str = str(int(year)) if (year is not None and str(year).strip().isdigit()) else None
    queries = []
    if year_str:
        queries.append(f"{title} {year_str} film")
        queries.append(f"{title} ({year_str} film)")
    queries.append(f"{title} (film)")
    queries.append(f"{title} film")
    queries.append(title)

    tried_pages = set()
    for q in queries:
        page_title = wiki_search_best_title(q)
        time.sleep(0.2)
        if not page_title or page_title in tried_pages:
            continue
        tried_pages.add(page_title)

        # 1) Check sections for plot
        sections = wiki_get_sections(page_title)
        if sections:
            for sec in sections:
                sec_line = sec.get("line", "").lower()
                if any(k in sec_line for k in ["plot", "synopsis", "summary", "plot summary"]):
                    txt = wiki_get_section_text(page_title, sec.get("index"))
                    if txt and len(txt) >= MIN_PLOT_CHARS:
                        return txt, page_title, "api_section"

        # 2) Fallback: parse full HTML
        try:
            params = {"action": "parse", "page": page_title, "prop": "text", "format": "json", "redirects": True}
            r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=12)
            html = r.json().get("parse", {}).get("text", {}).get("*", "")
            if html:
                soup = BeautifulSoup(html, "html.parser")
                span = soup.find("span", id=lambda v: v and "plot" in v.lower())
                if span:
                    header = span.find_parent(["h2","h3"])
                    if header:
                        paras = []
                        for sib in header.find_next_siblings():
                            if sib.name in ["h2","h3"]:
                                break
                            if sib.name == "p":
                                txt = sib.get_text(" ", strip=True)
                                txt = re.sub(r'\[\d+\]', '', txt)
                                if len(txt) > 30:
                                    paras.append(txt)
                        if paras:
                            text = " ".join(paras)
                            if len(text) >= MIN_PLOT_CHARS:
                                return text, page_title, "api_full_html"
        except Exception:
            pass

        # 3) HTML fallback via constructed URLs
        possible_urls = [
            f"https://en.wikipedia.org/wiki/{quote(page_title.replace(' ', '_'))}",
            f"https://en.wikipedia.org/wiki/{quote(title.replace(' ', '_'))}_(film)"
        ]
        if year_str:
            possible_urls.insert(0, f"https://en.wikipedia.org/wiki/{quote(title.replace(' ', '_'))}({year_str}_film)")
            possible_urls.insert(0, f"https://en.wikipedia.org/wiki/{quote(title.replace(' ', '_'))}_({year_str}_film)")
        for url in possible_urls:
            txt = extract_plot_from_html(url)
            time.sleep(0.15)
            if txt and len(txt) >= MIN_PLOT_CHARS:
                inferred = url.split("/wiki/")[-1].replace("_", " ")
                return txt, inferred, "html_fallback"

    return None, None, "none"

# ====================================
# RUN EXTRACTION
# ====================================
input_file = "movies_input.xlsx"
if not os.path.exists(input_file):
    raise FileNotFoundError(f"Input file not found: {input_file}")

df = pd.read_excel(input_file)

plots, pages, methods = [], [], []
success = 0
total = len(df)

for idx, row in tqdm(df.iterrows(), total=total):
    raw_title = row.get("Title", "")
    year = row.get("Year", None) if "Year" in df.columns else None

    plot_text, used_page, method = find_plot_for_title(raw_title, year)
    plots.append(plot_text)
    pages.append(used_page)
    methods.append(method)

    if plot_text:
        success += 1

    if (idx + 1) % CHECKPOINT_INTERVAL == 0:
        df_partial = df.copy()
        df_partial["Wikipedia Plot"] = plots + [None] * (len(df) - len(plots))
        df_partial["Wiki Page Used"] = pages + [None] * (len(df) - len(pages))
        df_partial["Method"] = methods + [None] * (len(df) - len(methods))
        cp_file = f"Movies_Wikipedia_Plots_checkpoint_{idx+1}.xlsx"
        df_partial.to_excel(cp_file, index=False)

df_out = df.copy()
df_out["Wikipedia Plot"] = plots
df_out["Wiki Page Used"] = pages
df_out["Method"] = methods

output_file = "Movies_Wikipedia_Plots_FINAL.xlsx"
df_out.to_excel(output_file, index=False)
